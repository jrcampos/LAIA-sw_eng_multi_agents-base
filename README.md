# ğŸ§  AI Multi-Agent Software Engineering Tutorial

This tutorial teaches you how to use a multi-agent LLM system (powered by OpenHands) to:

- Generate specifications  
- Generate pytest tests  
- Implement Python code  
- Generate review reports  
- Maintain full traceability across SPEC â†’ TEST â†’ CODE â†’ REVIEW  

You will interact with the system through a CLI menu in `my_agents.py`.

---
# ğŸ¤– What Is OpenHands?

OpenHands is an **agentic framework** that enables LLMs to operate as structured software-development agents.  
It provides controlled tool usage (file editing, terminal execution, reasoning loops) inside a reproducible sandbox.

OpenHands comes in *multiple* forms, depending on how you want to use it:

### ğŸ–¥ï¸ 1. OpenHands Desktop GUI (Full Development Environment)
- A full graphical environment where an AI agent edits your repository.
- Includes a browser-based VS Code interface.
- Best for manual experiments or interactive agent development.

### â˜ï¸ 2. OpenHands Cloud (Hosted)
- A managed online environment that runs agents remotely.
- Useful when you need GPU acceleration or shared workspaces.
- Same agent capabilities, but running in the cloud.

### ğŸ§© 3. OpenHands SDK (Programmatic / Library Mode)
- A Python library (`openhands-sdk`, `openhands-tools`, `openhands-workspace`)
- Allows you to **embed OpenHands agents into your own application**.
- You define the agents, prompts, tools, and workflow.
- Agents run inside a sandboxed workspace, fully controlled by your code.

ğŸ‘‰ **This tutorial uses the SDK version.**

You are not opening the GUI.  
You are not connecting to the cloud.

Instead, the project integrates OpenHands **directly into Python code**. 

---

# ğŸ¤– What Are Agentic Systems?

Agentic systems extend LLMs beyond chat responses by giving them:

- **Tools** (file editor, terminal, workspace)  
- **Role-specific instructions**  
- **Persistent context**  
- **Structured multi-step execution**  

Instead of returning text, agents can:

- Read/write files  
- Modify code  
- Run pytest  
- Inspect the workspace  
- Produce review reports  

This enables full automation of software engineering workflows.

---

# ğŸ“ Project Structure Overview

This project has a series of agents to support the development of a calculator project. Your repository has the following architecture:

```
workspace/
â”‚
â”œâ”€â”€ docs/                  # Project-wide prompt
â”œâ”€â”€ reviews/               # Reviewer-generated code reviews
â”œâ”€â”€ specs/                 # Analyst-generated specifications
â”œâ”€â”€ src/                   # Coder-generated Python implementation
â”œâ”€â”€ tests/                 # Tester-generated pytest tests
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ analyst.*
â”‚   â”œâ”€â”€ tester.*
â”‚   â”œâ”€â”€ coder.*
â”‚   â””â”€â”€ reviewer.*
â”‚
â”œâ”€â”€ requirements.txt       # CLI interface
â”œâ”€â”€ my_agents.py           # CLI interface
â””â”€â”€ Dockerfile             # Execution environment
```

The entire pipeline forms an iterative loop:

```
SPEC â†’ TESTS â†’ CODE â†’ REVIEW â†’ (repeat)
```

---

# ğŸŸ¦ 1st Step â€” Explore the Already-Implemented â€œAddâ€ Feature

Before using any agents, explore the **existing Add functionality** of the calculator program.

You will find:

```
specs/ft-add.md            â†’ Analyst-created specification
tests/add/                 â†’ Tester-generated test suite
src/calculator.py          â†’ Coderâ€™s implementation
reviews/rv-add.md          â†’ Reviewerâ€™s analysis (if present)
```

Open these files and observe how they relate:

1. **Specification** defines Acceptance Criteria  
2. **Tester** generates one test file per AC  
3. **Coder** implements code until tests pass  
4. **Reviewer** analyzes the entire chain and generates traceability

This is the blueprint for all features you will develop.

---

# âš™ï¸ Configuring the `.env` File

The multi-agent system requires a `.env` file so the agents know **which LLM to call**, **where the API is hosted**, and **how to communicate with your remote LLM (e.g., ChatGPT or Ollama server)**.

You must create a `.env` file in the **project root** with the following structure:

```
# --- LLM CONFIG (required) ---

# The name of the model you pulled into the remote Ollama server.
# Example: ollama pull devstral
# !NOTE: the context window needs to be large enough, e.g., 32k otherwise it will not properly configure the tool calls!
LLM_MODEL=ollama/devstral-32k:latest
# LLM_MODEL=openai/gpt-4.1

# The OpenAI-compatible API endpoint exposed by the remote Ollama container.
# This should point to your remote machine, VPN-accessible host, or LAN host.
LLM_API_BASE=YOUR_IP_HERE
# LLM_API_BASE=https://api.openai.com/v1

# Ollama ignores API keys, but the OpenHands SDK requires the field.
# You may put any string here.
LLM_API_KEY=dummy
# LLM_API_KEY=YOUR_CHATGPT_KEY_HERE
```

This works as well for ChatGPT or similar, just replace the URL and use your key

---

# ğŸ³ 1. Running the Project Inside Docker

OpenHands requires a controlled reproducible environment, so everything is run in Docker.

## âœ” 1.1 Build the Docker image
On root of the project run this. This will create a new image based on the Dockerfile with the OpenHands SDK.

```
docker build -t ai-se-agents .
```

## âœ” 1.2 Run the container

This mounts your project directory as `/workspace`. This will create a volume that shares your project (so you can edit your code with your favorite IDE) and we then execute the code through a terminal to the container. **Run this in the root folder of the project.**

```
docker run -it --rm \
  -v "$(pwd)":/workspace \
  -w /workspace \
  ai-se-agents bash
```

You will now be inside the isolated development environment. You will run your code from here! i.e., **python my_agents.py**

---

# â–¶ï¸ 2. Start the Multi-Agent Application

Inside the container:

```
python my_agents.py
```

You will see:

```
==============================================
  AI Multi-Agent Development Application
==============================================
Active Spec: None selected
----------------------------------------------
1) Choose active specification
2) Generate or update a specification (Analyst)
3) Generate or replace tests (Developer Tests)
4) Implement or update code (Developer Code)
5) Review implementation and tests (Reviewer)
6) Exit
----------------------------------------------
```

---

# ğŸ§ª 3. Warm-Up Task: Subtraction Feature (Included)

Your repository already contains:

```
specs/ft-subtract.md
```

Your job:

1. Select the spec  
2. Generate tests  
3. Implement code  
4. Generate a review  

Letâ€™s go step-by-step.

---

# Step 1 â€” Select the subtraction spec

Start the app:

```
python my_agents.py
```

Choose:

```
1) Choose active specification
```

Select:

```
ft-subtract.md
```

---

# Step 2 â€” Generate tests for subtraction

Choose:

```
3) Generate or replace tests (Developer Tests)
```

This creates:

```
tests/subtract/
```

with test files:

```
ac_01_*.py
ac_02_*.py
...
ac_05_*.py
```

Each test corresponds to one Acceptance Criterion.

---

# Step 3 â€” Implement subtraction

Choose:

```
4) Implement or update code (Developer Code)
```

The Coder agent:

- Reads the tests  
- Reads the spec  
- Modifies only `src/calculator.py`  
- Runs pytest  
- Repeats until **all tests pass**  

---

# Step 4 â€” Generate a review

Choose:

```
5) Review implementation and tests (Reviewer)
```

This produces:

```
reviews/rv-subtract.md
```

The review includes:

- Summary  
- Issues & risks  
- Alignment with specification  
- A full traceability matrix  
- Recommendations  

---

# ğŸ§® 4. Student Task: Add Multiplication Functionality

Now you repeat the entire flow yourself.

## 4.1 Use the Analyst to generate a spec

Choose:

```
2) Generate or update a specification (Analyst)
```

Example input:

```
Create a full software specification for adding multiplication functionality to the Calculator class. Include functional requirements, acceptance criteria, inputs/outputs, and edge cases.
```

A file will be created:

```
specs/ft-multiply.md
```

## 4.2 Select the new specification

```
1) Choose active specification
```

Pick:

```
ft-multiply.md
```

## 4.3 Generate tests

```
3) Generate or replace tests (Developer Tests)
```

Produces:

```
tests/multiply/
```

## 4.4 Implement code

```
4) Implement or update code (Developer Code)
```

Coder modifies:

```
src/calculator.py
```

until all tests pass.

## 4.5 Generate review

```
5) Review implementation and tests (Reviewer)
```

Review appears under:

```
reviews/rv-multiply.md
```

---

# â— 5. Student Task: Implement Division

Repeat the exact same workflow:

- Generate the division specification  
- Activate it  
- Generate tests  
- Implement code  
- Generate a reviewer report  
- Compare the traceability matrix with previous features  

---

# ğŸ“˜ 6. What You Should Observe

As you complete each feature, pay attention to:

### ğŸ§© Analyst
- How clear the functional requirements are  
- How acceptance criteria are structured  

### ğŸ§ª Tester
- How ACs turn into files  
- How one AC becomes one pytest file  

### ğŸ§‘â€ğŸ’» Coder
- How code evolves to satisfy tests  
- No code is ever written without tests  

### ğŸ“ Reviewer
- Completeness of test coverage  
- Whether code truly matches the spec  
- The traceability matrix linking AC â†’ Test â†’ Code  

---

# ğŸ”§ Optional Extension: Adding a Repo Manager Agent

This section introduces an optional advanced task for students:  
**designing a new â€œRepo Manager Agentâ€ responsible for repository maintenance.**

This teaches:
- Separation of responsibilities between agents  
- Safe automation of version control  
- Git workflows in multi-agent systems  
- How LLM agents can support DevOps tasks  
- How to create new agent prompts and integrate them into the application  

---

## ğŸ§  Why Add a Repo Agent?

Currently your system has:

- Analyst â†’ writes specifications  
- Tester â†’ generates tests  
- Coder â†’ writes source code  
- Reviewer â†’ evaluates quality  

But **no agent manages the repository**, meaning commits, diffs, and tagging must be done manually.

A Repo Manager Agent provides structure and repeatability.

---

## ğŸ¯ Goal of the Repo Manager Agent

The new agent should:

- Stage modified files  
- Commit them with a clean, structured commit message  
- Show diffs on request  
- Never modify code, tests, or specs  
- Only perform repository management tasks  

This mirrors real-world DevOps and multi-agent coding workflows.

---

# âœ… End of Tutorial

You now understand the complete multi-agent SE workflow powered by LLMs:

```
SPECIFICATION â†’ TESTS â†’ IMPLEMENTATION â†’ REVIEW â†’ TRACEABILITY
```

This pipeline enforces:

- High-quality specifications  
- Automated, deterministic test generation  
- Correct-by-construction implementation  
- Auditable traceability  
- Rigorous software engineering discipline  

Happy coding!  